
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import LabelEncoder


df = pd.read_csv("xAPI-Edu-Data.csv")

# Display basic information about the dataset
print("Dataset Information:")
print(df.info())
print("\nFirst 5 rows:")
print(df.head())
print("\nDescriptive Statistics:")
print(df.describe())

# Check for missing values
print("\nMissing Values:")
print(df.isnull().sum())

# Data Cleaning - handle missing values if any
df = df.dropna()  # Simple approach - remove rows with missing values

# Convert categorical target to numerical (if needed)
# For this example, we'll encode the 'Class' column
le = LabelEncoder()
df['Class_encoded'] = le.fit_transform(df['Class'])

# Select features and target variable
# Using numerical features from the dataset
X = df[['raisedhands', 'VisITedResources', 'AnnouncementsView', 'Discussion']]
y = df['Class_encoded']  # Using the encoded class as target

# Visualize the relationship between features and target
plt.figure(figsize=(15, 10))
for i, col in enumerate(X.columns):
    plt.subplot(2, 2, i+1)
    plt.scatter(X[col], y, alpha=0.5)
    plt.title(f'{col} vs Class')
    plt.xlabel(col)
    plt.ylabel('Class (encoded)')
plt.tight_layout()
plt.show()

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model's performance
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("\nModel Performance Metrics:")
print(f"Mean Absolute Error: {mae:.2f}")
print(f"Mean Squared Error: {mse:.2f}")
print(f"Root Mean Squared Error: {rmse:.2f}")
print(f"R-squared: {r2:.2f}")

# Visualize the predictions vs actual values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, color='blue', alpha=0.5)
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)  # Diagonal line
plt.title('Actual vs Predicted Class Values')
plt.xlabel('Actual Class (encoded)')
plt.ylabel('Predicted Class')
plt.grid(True)
plt.show()

# Print the regression equation coefficients
print("\nRegression Equation Coefficients:")
for feature, coef in zip(X.columns, model.coef_):
    print(f"{feature}: {coef:.4f}")
print(f"Intercept: {model.intercept_:.4f}")
